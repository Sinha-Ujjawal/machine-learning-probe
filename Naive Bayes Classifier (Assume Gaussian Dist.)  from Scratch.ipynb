{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "17bf104d-2527-4583-b32b-090466e39bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, NewType\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Using Welford's algorithm for efficient Mean and Variance calculation\n",
    "# Reference: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n",
    "class FeatureStats:\n",
    "    __slots__ = [\"n_features\", \"n\", \"means\", \"M2s\"]\n",
    "    def __init__(self, n_features: int):\n",
    "        self.n_features = n_features\n",
    "        self.n = 0\n",
    "        self.means = [0] * n_features\n",
    "        self.M2s= [0] * n_features\n",
    "\n",
    "    def update(self, xs: List[float]):\n",
    "        self.n += 1\n",
    "        for idx, x in enumerate(xs):\n",
    "            delta = x - self.means[idx]\n",
    "            self.means[idx] += delta / self.n\n",
    "            delta2 = x - self.means[idx]\n",
    "            self.M2s[idx] += delta * delta2\n",
    "\n",
    "    def logprob(self, xs: List[float]) -> float:\n",
    "        ret = 0\n",
    "        for idx in range(self.n_features):\n",
    "            x = xs[idx]\n",
    "            mean = self.means[idx]\n",
    "            M2 = self.M2s[idx]\n",
    "            if self.n <= 1:\n",
    "                var = 1e-6\n",
    "            else:\n",
    "                var = max(M2 / (self.n - 1), 1e-6)\n",
    "            logp = -0.5 * (math.log(2 * math.pi * var) + (x - mean) ** 2 / var)\n",
    "            ret += logp\n",
    "        return ret\n",
    "\n",
    "class GaussianNaiveBayesClassifierModel:\n",
    "    def __init__(self, X_train: List[List[float]], classes: List[int]):\n",
    "        class_counts = Counter(classes)\n",
    "        self.class_probs = {\n",
    "            cls: cnt / len(classes)\n",
    "            for cls, cnt in class_counts.items()\n",
    "        }\n",
    "        self.n_features = len(X_train[0])\n",
    "        self.feature_probs = defaultdict(lambda: FeatureStats(self.n_features))\n",
    "        for idx in range(len(X_train)):\n",
    "            features = X_train[idx]\n",
    "            cls = classes[idx]\n",
    "            self.feature_probs[cls].update(features)\n",
    "\n",
    "    def predict_cls_single(self, features: List[float]) -> int:\n",
    "        posteriors = {}\n",
    "        for cls, p in self.class_probs.items():\n",
    "            posterior = math.log(p)\n",
    "            posterior += self.feature_probs[cls].logprob(features)\n",
    "            posteriors[cls] = posterior\n",
    "        return max(posteriors, key=posteriors.get)\n",
    "        \n",
    "    def predict_cls(self, X: List[List[float]]) -> List[int]:\n",
    "        return [self.predict_cls_single(features) for features in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "393b2508-d0ad-416b-8857-7137088e9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Tuple\n",
    "\n",
    "class BagOfWords:\n",
    "    def __init__(self, stopwords=None):\n",
    "        self.stopwords = stopwords or set()\n",
    "        self.stopwords = set(self.stopwords)\n",
    "        self.vocabs = OrderedDict()\n",
    "\n",
    "    def update(self, *texts: Tuple[str]):\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                if word in self.stopwords:\n",
    "                    continue\n",
    "                if word not in self.vocabs:\n",
    "                    self.vocabs[word] = len(self.vocabs)\n",
    "\n",
    "    def vectorize(self, *texts: Tuple[str]) -> List[int]:\n",
    "        ret = []\n",
    "        for text in texts:\n",
    "            vector = [0] * len(self.vocabs)\n",
    "            for word, cnt in Counter(text.split()).items():\n",
    "                if word in self.stopwords:\n",
    "                    continue\n",
    "                if not word in self.vocabs:\n",
    "                    continue\n",
    "                vector[self.vocabs[word]] = cnt\n",
    "            ret.append(vector)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd3ba731-3212-4b0e-abbc-3219c06acd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/enron/enron2.tar.gz', 'dataset/enron/enron6.tar.gz', 'dataset/enron/enron4.tar.gz', 'dataset/enron/enron1.tar.gz', 'dataset/enron/enron3.tar.gz', 'dataset/enron/enron5.tar.gz']\n",
      "Done reading from dataset/enron/enron2.tar.gz\n",
      "Done reading from dataset/enron/enron6.tar.gz\n",
      "Done reading from dataset/enron/enron4.tar.gz\n",
      "Done reading from dataset/enron/enron1.tar.gz\n",
      "Done reading from dataset/enron/enron3.tar.gz\n",
      "Done reading from dataset/enron/enron5.tar.gz\n",
      "len(texts)=33722\n",
      "len(classes)=33722\n"
     ]
    }
   ],
   "source": [
    "import tarfile as tf\n",
    "from glob import glob\n",
    "\n",
    "enron_files = glob(\"dataset/enron/enron*.tar.gz\")\n",
    "print(enron_files)\n",
    "\n",
    "texts = []\n",
    "classes = []\n",
    "\n",
    "for enron_file in enron_files:\n",
    "    with tf.open(enron_file) as fp:\n",
    "        for member in fp.getmembers():\n",
    "            if not member.isfile():\n",
    "                continue\n",
    "            cls = int(member.name.split(\"/\")[1] == \"spam\")\n",
    "            text = (\n",
    "                fp.extractfile(member)\n",
    "                .read()\n",
    "                .decode(\"latin-1\")\n",
    "                .strip()\n",
    "                .lower()\n",
    "            )\n",
    "            texts.append(text)\n",
    "            classes.append(cls)\n",
    "    print(f\"Done reading from {enron_file}\")\n",
    "\n",
    "print(f\"{len(texts)=}\")\n",
    "print(f\"{len(classes)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74a12b5b-69c9-43d7-9804-4d20002e1ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 17171, 0: 16551})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c98ce07-f894-427c-86c1-28541e29377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove emails, URLs, numbers\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
    "    text = re.sub(r'http\\S+', '', text)   # Remove URLs\n",
    "    text = re.sub(r'\\d+', '', text)       # Remove numbers\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Common English stop words\n",
    "STOP_WORDS = set(open(\"./dataset/english_stopwords.txt\").readlines())\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join(word for word in text.split() if word not in STOP_WORDS)\n",
    "\n",
    "def filter_by_length(text, min_len=3, max_len=15):\n",
    "    words = text.split()\n",
    "    filtered = [word for word in words if min_len <= len(word) <= max_len]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def preprocess_for_vocab_reduction(texts, min_len=3, max_len=15):\n",
    "    # Step 1: Clean and preprocess\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        # Clean\n",
    "        text = clean_text(text)\n",
    "        # Remove stop words\n",
    "        text = remove_stopwords(text)\n",
    "        # Filter by length\n",
    "        text = filter_by_length(text, min_len, max_len)\n",
    "        processed_texts.append(text)\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13120e16-335b-433e-b62a-71724b249749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, split=0.8):\n",
    "    assert len(X) == len(y), f\"Size of X and y must be same, got: {len(X)=}, {len(y)=}\"\n",
    "    n = len(X)\n",
    "    train_size = math.ceil(n*split)\n",
    "    return X[:train_size], y[:train_size], X[train_size:], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9f44799e-68b7-4a01-bb47-80dafb6601ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_cleaned = preprocess_for_vocab_reduction(texts, min_len=3, max_len=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bedb644f-4bda-4190-87e2-36e08ed082b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)=26978\n",
      "len(y_train)=26978\n",
      "len(X_test)=6744\n",
      "len(y_test)=6744\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(texts_cleaned, classes)\n",
    "print(f\"{len(X_train)=}\")\n",
    "print(f\"{len(y_train)=}\")\n",
    "print(f\"{len(X_test)=}\")\n",
    "print(f\"{len(y_test)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "15e8abbe-abbd-4bc6-8b99-5745c42d1683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70833"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = BagOfWords()\n",
    "bow.update(*X_train)\n",
    "len(bow.vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "be59340e-846a-44ac-8e3c-aedc1cc494c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = bow.vectorize(*X_train)\n",
    "X_test_vectorized = bow.vectorize(*X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba0da5ad-258c-44ac-ac9f-b2fdf4e3f0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GaussianNaiveBayesClassifierModel at 0x11060aec0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNaiveBayesClassifierModel(X_train_vectorized, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f1eced71-271c-47ff-a17d-cc072b4bb231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_cls(X_test_vectorized[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c442a1cc-36ed-4ffa-995d-f9642657a534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4a1da1b3-5cc5-4552-a347-d4f391654d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_cls(X_test_vectorized[-25:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de6a0bbb-6e0a-43b2-846f-e8ab260dcc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834cdbc-23ec-4bf6-ba16-b822d0363968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
